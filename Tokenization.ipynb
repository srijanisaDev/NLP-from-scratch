{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58588daa",
   "metadata": {},
   "source": [
    "## TOKENIZATION -> converting the corpus/paragraph into sentences....So how this works is that we look for characters like \".\" full stop/ commas and then \n",
    "   ## segregate them into separate sentences\n",
    "\n",
    "\n",
    "\n",
    "## Now we can futher perform tokenization on sentences and this will convert them into words .....and then after futher processing we convert the words into VECTORS. which play a very important role in deep learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21de472b",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\" Hello i am srijan. I am studying tokenization in NLP right now  . I am really excited about it .\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cde34c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello i am srijan. I am studying tokenization in NLP right now  . I am really excited about it .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c49fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenization\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b77125e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = sent_tokenize(corpus)   ## sent tokenize returns a list of sentence sfrom a paragraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1d703bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Hello i am srijan.\n",
      "I am studying tokenization in NLP right now  .\n",
      "I am really excited about it .\n"
     ]
    }
   ],
   "source": [
    "for sentence in documents:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e1d842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## now tokenizing from word or paragraph we use word_tokeinzer\n",
    "\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39d4a2ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'i',\n",
       " 'am',\n",
       " 'srijan',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'studying',\n",
       " 'tokenization',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'right',\n",
       " 'now',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'really',\n",
       " 'excited',\n",
       " 'about',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7615ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'i', 'am', 'srijan', '.']\n",
      "['I', 'am', 'studying', 'tokenization', 'in', 'NLP', 'right', 'now', '.']\n",
      "['I', 'am', 'really', 'excited', 'about', 'it', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentences in documents:\n",
    "    print(word_tokenize(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f55fa186",
   "metadata": {},
   "outputs": [],
   "source": [
    "## there is this another library we use when we want to tokenize the punctuation marks also\n",
    "## this library is known as wordpunct_tokenize\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "512bef9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'i',\n",
       " 'am',\n",
       " 'srijan',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'studying',\n",
       " 'tokenization',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'right',\n",
       " 'now',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'really',\n",
       " 'excited',\n",
       " 'about',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordpunct_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5506bd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TreebankwordTokenizer.... so in this type of tokenizer there is a very minute difference between the word_punct tokenizer\n",
    "## the difference is that in word_punct_tokeinzer every punctuation is treated safely but in treebank when a punctuation occurs then \n",
    "## it uses the last word attached to it as a complete token\n",
    "\n",
    "\n",
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "20f05996",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer  = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8a6fd5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'i',\n",
       " 'am',\n",
       " 'srijan.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'studying',\n",
       " 'tokenization',\n",
       " 'in',\n",
       " 'NLP',\n",
       " 'right',\n",
       " 'now',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'really',\n",
       " 'excited',\n",
       " 'about',\n",
       " 'it',\n",
       " '.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8485b2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
